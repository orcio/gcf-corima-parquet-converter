name: Deploy & Test Cloud Function Conversion

on:
  workflow_dispatch:
  push:
    branches: [ master ]

# ▸ Secrets richiesti:
#   - GCP_PROJECT       → ID del progetto GCP
#   - GCP_SA_KEY        → chiave JSON del service-account
#   - GCS_BUCKET        → nome del bucket (solo nome, senza gs://)

env:
  REGION: europe-west1

jobs:
# -------------------------------------------------------------------
# 1) DEPLOY (o update) DELLA FUNZIONE
# -------------------------------------------------------------------
  deploy-function:
    runs-on: ubuntu-latest
    env:
      BUCKET_NAME: ${{ secrets.GCS_BUCKET }}

    steps:
      - uses: actions/checkout@v3

      - name: Auth to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          export_default_credentials: true

      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT }}
          install_components: 'beta'   # per funzioni 2ª gen, togli se usi gen1

      # ▸ DEPLOY / UPDATE
      - name: Deploy Cloud Function
        run: |
          gcloud functions deploy process_dat_to_parquet \
            --quiet \
            --runtime python310 \
            --entry-point process_dat_to_parquet \
            --trigger-event google.storage.object.finalize \
            --trigger-resource ${BUCKET_NAME} \
            --memory 2048MB \
            --region ${REGION} \
            --source .               # cartella radice del repo


# -------------------------------------------------------------------
# 2) TEST DI CONVERSIONE (parte solo se deploy ok)
# -------------------------------------------------------------------
  test-conversion:
    needs: deploy-function          # <-- dipende dal job precedente
    runs-on: ubuntu-latest
    env:
      BUCKET_NAME: ${{ secrets.GCS_BUCKET }}

    steps:
      - uses: actions/checkout@v3

      - name: Auth to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
          export_default_credentials: true

      - name: Setup gcloud / gsutil
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT }}
          install_components: 'gsutil'

      - name: Verify bucket exists
        run: gsutil ls -b gs://${BUCKET_NAME}

      # ▸ 1. Copia sample per triggerare la funzione
      - name: Upload test files
        id: upload
        run: |
          TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)
          TEST_PREFIX="dati_raw/test_${TIMESTAMP}"

          SAMPLE_PREFIX="dati_raw/LAF1_20250711_16_22_17"

          gsutil -m cp "gs://${BUCKET_NAME}/${SAMPLE_PREFIX}/*.dat" \
                        "gs://${BUCKET_NAME}/${TEST_PREFIX}/"
          gsutil -m cp "gs://${BUCKET_NAME}/${SAMPLE_PREFIX}/*.json" \
                        "gs://${BUCKET_NAME}/${TEST_PREFIX}/"

      # ▸ 2. Attesa output Parquet
      - name: Wait for Parquet  
        run: | 
          parquet_folder="data_parquet"
          alias="LAF1"
          dt="2025/07/11"
          DEST_DIR="${parquet_folder}/${alias}/${dt}"      # cartella dove compariranno i file

          for i in {1..60}; do
            # cerca il primo parquet che comincia con iis3dwb_acc_
            FILE=$(gsutil ls "gs://${BUCKET_NAME}/${DEST_DIR}/iis3dwb_acc_*.parquet" 2>/dev/null | head -n 1)
            if [[ -n "$FILE" ]]; then
              echo "Found $FILE"
              # esporta solo il path relativo per lo step successivo
              DEST_PATH=${FILE#gs://${BUCKET_NAME}/}
              echo "DEST_PATH=${DEST_PATH}" >> $GITHUB_ENV
              exit 0
            fi
            sleep 10
          done
          echo "Parquet not found"; exit 1

      # ▸ 3. Validazione Parquet
      - name: Install pandas & pyarrow
        run: pip install --quiet pandas pyarrow
       
      - name: Validate Parquet
        run: |
          gsutil cp "gs://${BUCKET_NAME}/${DEST_PATH}" output.parquet
          python -c "import pandas as pd, sys, pyarrow.parquet as pq; df = pd.read_parquet('output.parquet'); assert 'alias' in df.columns; assert df['Time'].dtype == 'int64'; print('✅ rows:', len(df))"
