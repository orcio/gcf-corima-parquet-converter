name: Test Cloud Function Conversion

# Puoi lanciarlo manualmente o su ogni push su main
on:
  workflow_dispatch:
  push:
    branches: [ main ]

env:
  # Solo il nome del bucket, senza gs://
  BUCKET_NAME: ${{ secrets.GCS_BUCKET }}

jobs:
  test-conversion:
    runs-on: ubuntu-latest
    steps:

      # 1) Prendi il codice
      - name: Checkout repo
        uses: actions/checkout@v3

      # 2) Autentica gcloud (e con gsutil viene installato insieme)
      - name: Setup Google Cloud SDK
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT }}
          service_account_key: ${{ secrets.GCP_SA_KEY }}
          export_default_credentials: true

      # 3) Debug (facoltativo): controlla che BUCKET_NAME sia corretto
      - name: Echo bucket
        run: echo "Bucket: gs://${BUCKET_NAME}"

      # 4) Copia i raw sample per triggerare la Function
      - name: Upload test files
        id: upload
        run: |
          # timestamp univoco per il test
          TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)
          TEST_PREFIX="dati_raw/test_${TIMESTAMP}"

          # cartella di esempio già presente in GCS
          SAMPLE_PREFIX="dati_raw/corima-data/LAF1_20250711_16_22_17"

          # copia .dat + .json nella nuova cartella di test
          gsutil -m cp "gs://${BUCKET_NAME}/${SAMPLE_PREFIX}/*.dat" \
                    "gs://${BUCKET_NAME}/${TEST_PREFIX}/"
          gsutil -m cp "gs://${BUCKET_NAME}/${SAMPLE_PREFIX}/*.json" \
                    "gs://${BUCKET_NAME}/${TEST_PREFIX}/"

          # esponi il TEST_PREFIX alle step successive
          echo "TEST_PREFIX=${TEST_PREFIX}" >> $GITHUB_ENV

      # 5) Attendi che il Parquet arrivi nel path costruito da alias/YYYY/MM/DD
      - name: Wait for output Parquet
        id: wait
        run: |
          # ricava alias e data dal SAMPLE_PREFIX
          SAMPLE_FOLDER=${SAMPLE_PREFIX##*/}       # es. "LAF1_20250711_16_22_17"
          alias=${SAMPLE_FOLDER%%_*}               # -> "LAF1"
          date_part=${SAMPLE_FOLDER#*_}            # -> "20250711_16_22_17"
          yyyymmdd=${date_part%%_*}                # -> "20250711"
          dt="${yyyymmdd:0:4}/${yyyymmdd:4:2}/${yyyymmdd:6:2}"

          DEST_PATH="${alias}/${dt}/iis3dwb_acc.parquet"
          echo "DEST_PATH=${DEST_PATH}" >> $GITHUB_ENV

          # loop di polling: 30 tentativi a 10s
          for i in {1..30}; do
            if gsutil -q stat "gs://${BUCKET_NAME}/${DEST_PATH}"; then
              echo "Found gs://${BUCKET_NAME}/${DEST_PATH}"
              exit 0
            fi
            sleep 10
          done

          echo "ERROR: Parquet not found after timeout"
          exit 1

      # 6) Scarica e verifica il Parquet
      - name: Validate Parquet
        run: |
          gsutil cp "gs://${BUCKET_NAME}/${DEST_PATH}" output.parquet

          python3 - <<EOF
            import pandas as pd

            df = pd.read_parquet("output.parquet")
            # Verifiche minime
            assert "alias" in df.columns, "Missing 'alias'"
            assert df["Time"].dtype == "int64", "Time must be int64"
            print("✅ Parquet OK:", df.head())
            EOF
