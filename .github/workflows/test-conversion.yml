name: Test Cloud Function Conversion

# Puoi lanciarlo manualmente o su ogni push
on:
  workflow_dispatch:
  push:
    branches: [ main ]

env:
  # Nome del tuo bucket GCS (impostalo nei Secrets o direttamente qui)
  BUCKET_NAME: ${{ secrets.GCS_BUCKET }}

jobs:
  test-conversion:
    runs-on: ubuntu-latest
    steps:

      # 1) Checkout del codice
      - name: Checkout repo
        uses: actions/checkout@v3

      # 2) Setup Google Cloud SDK e autenticazione
      - name: Configure gcloud
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT }}
          service_account_key: ${{ secrets.GCP_SA_KEY }}
          export_default_credentials: true
    
      - name: Debug ENV
        run: |
            echo "BUCKET_NAME is: '$BUCKET_NAME'"


      # 3) Copia un file di esempio nel percorso dati_raw per triggerare la Function
      - name: Trigger conversion function
        id: trigger
        run: |
          TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)
          TEST_PREFIX="dati_raw/test_${TIMESTAMP}"
          # Qui il path sorgente che contiene il raw di esempio:
          SAMPLE_PATH="dati_raw/corima-data/LAF1_20250711_16_22_17"
          # Copia tutto (.dat + .json) nel prefix di test
          gsutil cp gs://$BUCKET_NAME/$SAMPLE_PATH/*.dat \
                    gs://$BUCKET_NAME/${TEST_PREFIX}/
          gsutil cp gs://$BUCKET_NAME/$SAMPLE_PATH/*.json \
                    gs://$BUCKET_NAME/${TEST_PREFIX}/
          echo "TEST_PREFIX=$TEST_PREFIX" >> $GITHUB_ENV


      # 4) Attendi che la Function elabori e scriva il Parquet nel path atteso
      - name: Wait for output Parquet
        id: wait
        run: |
          # Estrai alias e data dal TEST_PREFIX
          folder_name=$(basename $TEST_PREFIX)              # es. test_20250711_16...
          # Se la cartella originale era "LAF1_20250711_16_22_17" potresti:
          SAMPLE_FOLDER="LAF1_20250711_16_22_17"
          alias=${SAMPLE_FOLDER%%_*}                        # -> "LAF1"
          date_part=${SAMPLE_FOLDER#*_}                     # -> "20250711_16_22_17"
          yyyymmdd=${date_part%%_*}                         # -> "20250711"
          dt="${yyyymmdd:0:4}/${yyyymmdd:4:2}/${yyyymmdd:6:2}"
          DEST_PATH="${alias}/${dt}/iis3dwb_acc.parquet"
          echo "Looking for gs://$BUCKET_NAME/$DEST_PATH"
          for i in {1..30}; do
            if gsutil -q stat gs://$BUCKET_NAME/$DEST_PATH; then
              echo "DEST_PATH=$DEST_PATH" >> $GITHUB_ENV
              exit 0
            fi
            sleep 10
          done
          exit 1

      # 5) Scarica e verifica contenuto del Parquet
      - name: Validate Parquet contents
        run: |
          gsutil cp gs://$BUCKET_NAME/${{ env.DEST_PATH }} ./output.parquet
          python3 - <<EOF

            import pandas as pd
            df = pd.read_parquet("output.parquet")
            # Controlli di base
            assert "alias" in df.columns, "Colonna 'alias' mancante"
            assert df["Time"].dtype == "int64", "Colonna 'Time' non è int64"
            print("✅ Parquet contiene le colonne 'alias' e 'Time' in nanosecondi")
            EOF
